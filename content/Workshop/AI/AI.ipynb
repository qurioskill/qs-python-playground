{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c7972ce1-d602-4f48-9ffa-fc217ba04402",
      "cell_type": "code",
      "source": "import json\nfrom pyodide.http import pyfetch   # async fetch wrapper\n\nasync def chat_with_llm(prompt: str) -> str:\n    \"\"\"\n    Send a JSON payload {\"prompt\": …} to the Flask /chat endpoint\n    and return the server's textual reply.\n    \"\"\"\n    resp = await pyfetch(\n        url=\"https://flask-test-gog5.onrender.com/chat\",\n        method=\"POST\",\n        headers={\"Content-Type\": \"application/json\"},\n        body=json.dumps({\"prompt\": prompt})\n    )\n\n    if resp.status != 200:\n        # surface server-side errors clearly\n        raise RuntimeError(f\"{resp.status}: {await resp.string()}\")\n\n    data = await resp.json()          # ⇦ expects {\"response\": \"...\"}\n    return data.get(\"response\", data)['answer'] # adjust key name if different",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "76826b16-fe71-49e4-90a0-1288ea08e864",
      "cell_type": "code",
      "source": "# Simple ChatGPT like wrapper\nquery = input(\"What I can help you with?\")\nreply = await chat_with_llm(query)\nprint(reply)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "389ac432-d0ed-4303-8dfd-9b4ef89d353b",
      "cell_type": "code",
      "source": "# Customer Service Agent version 1\n\nprompt = \"You are an experienced Customer Service Agent. Rate the customer complains as High/Medium/Low. Return one of the three options. \\nQuery:\"\n\nquery = input(\"Enter your grievance:\\n\")\n\nllm_query = prompt + query\n\nreply = await chat_with_llm(llm_query)\n\nif reply == \"High\":\n    print(\"This is a high priority request\")\nif reply == \"Medium\":\n    print(\"This is a medium priority request\")\nif reply == \"Low\":\n    print(\"This is a low priority request\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "70834107-f48d-428f-885e-82abbe0f704e",
      "cell_type": "code",
      "source": "# Customer Service Agent version 2 - External Data\ntypes_of_request = [\n    {\n        \"type\": \"Computer is being slow/not started\",\n        \"priority\": \"Medium\"\n    },\n    {\n        \"type\": \"Computer is heating up\",\n        \"priority\": \"High\"\n    },\n    {\n        \"type\": \"Some applications cannot be installed\",\n        \"priority\": \"Low\"\n    }\n]\n\n\nquery = input(\"Enter your grievance:\\n\")\n\nprompt = f'''You are an experienced Customer Service Agent. \nRate the customer complains as High/Medium/Low. Return one of the three options. \nUse the priority list as reference.\nQuery:{query}\nReference:{types_of_request}'''\n\nreply = await chat_with_llm(prompt)\n\nif reply == \"High\":\n    print(\"This is a high priority request\")\nif reply == \"Medium\":\n    print(\"This is a medium priority request\")\nif reply == \"Low\":\n    print(\"This is a low priority request\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "87553177-4fcd-4926-9919-95fc0d3dbbb6",
      "cell_type": "code",
      "source": "# Task: Customer Service Agent version 3 - Providing Solutions\ntypes_of_request = [\n    {\n        \"type\": \"Computer is being slow/not started\",\n        \"response_context\": \"These kind of requests are medium priority. Provide a limited detailed response.\"\n    },\n    {\n        \"type\": \"Computer is heating up\",\n        \"response_context\": \"These kind of requests are high priority. Provide a detailed response.\"\n    },\n    {\n        \"type\": \"Some applications cannot be installed\",\n       \"response_context\": \"These kind of requests are low priority. Provide a vague response response.\"\n    }\n]\n\n\nquery = input(\"Enter your grievance:\\n\")\n\nprompt = f'''You are an experienced Customer Service Agent. Provide answers to users queries in an informative tone. \nUse the reference to generate your responses.\nQuery:{query}\nReference:{types_of_request}'''\n\nreply = await chat_with_llm(prompt)\n\nprint(reply)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "550aca91-355e-475a-be62-8c35bd9da026",
      "cell_type": "code",
      "source": "# Task: Creaft a marketing specialist prompt to make the following AI agent take an input of a product and write a linkedin post based on that product.\n\nprompt = \"FILL THIS\"\n\nproduct = input(\"Enter the product you would like to sell on linkedin\")\n\nfinal_prompt = f\"{prompt} \\n Product: {product}\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}